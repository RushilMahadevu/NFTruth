{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4134b3e",
   "metadata": {},
   "source": [
    "#  **NFT Authenticity Model - Chronological Flow** (Made by Claude Sonnet 4)\n",
    "\n",
    "\n",
    "## 1. **Initialization Phase** (`__init__`)\n",
    "When the `NFTAuthenticityModel` is created:\n",
    "- Sets up empty dictionaries to store different ML models (`self.models`)\n",
    "- Initializes storage for model performance scores and results\n",
    "- Creates a `StandardScaler` for feature normalization\n",
    "- Defines the path where the final model will be saved\n",
    "\n",
    "## 2. **Data Loading Phase** (`prepare_data()`)\n",
    "The system looks for training data:\n",
    "- Searches the `training_data/` directory for JSON files with NFT data\n",
    "- Finds the most recent file (sorted by filename)\n",
    "- Loads the JSON data containing NFT collection features\n",
    "- Extracts feature arrays and collection names from each record\n",
    "- Converts everything into a pandas DataFrame for easier manipulation\n",
    "\n",
    "## 3. **Label Creation Phase** (`create_synthetic_labels()`)\n",
    "Since there's no ground truth about which NFTs are legitimate vs suspicious:\n",
    "- **Creates synthetic labels using a scoring system**:\n",
    "  - +3 points: Verified collection\n",
    "  - +1 point each: Has Discord, has Twitter\n",
    "  - +2 points: Has floor price > 0\n",
    "  - +2 points: High trading volume (>1000)\n",
    "  - +1 point: Many owners (>1000)\n",
    "  - +1 point: Reddit mentions exist\n",
    "  - +3 points: Collection is in known legitimate list\n",
    "- **Labels collections as legitimate (1) if score ≥ 6, suspicious (0) otherwise**\n",
    "\n",
    "## 4. **Feature Engineering Phase** (`engineer_features()`)\n",
    "Transforms raw data into more meaningful features:\n",
    "- **Volume per owner**: `total_volume / num_owners` (liquidity indicator)\n",
    "- **Market cap to volume ratio**: `market_cap / total_volume` (market efficiency)\n",
    "- **Price premium**: `average_price / floor_price` (pricing structure)\n",
    "- **Social engagement score**: `reddit_mentions + reddit_engagement`\n",
    "- **Liquidity indicator**: `√(total_volume × num_owners)` (market activity)\n",
    "- Handles division by zero and infinite values safely\n",
    "\n",
    "## 5. **Data Augmentation Phase** (`generate_synthetic_data()`)\n",
    "If the dataset is too small (<10 samples):\n",
    "- **Generates 70% legitimate collections** with characteristics like:\n",
    "  - High verification rate, strong social media presence\n",
    "  - Higher floor prices, market caps, and trading volumes\n",
    "  - More owners and positive Reddit sentiment\n",
    "- **Generates 30% suspicious collections** with characteristics like:\n",
    "  - Low verification rate, weak social media presence\n",
    "  - Lower prices, volumes, and fewer owners\n",
    "  - Less Reddit engagement and more negative sentiment\n",
    "\n",
    "## 6. **Model Training Phase** (`train_models()`)\n",
    "Trains four different machine learning algorithms:\n",
    "\n",
    "### Data Splitting:\n",
    "- Splits data 80/20 for training/testing (if enough data exists)\n",
    "- Uses stratified sampling to maintain class balance\n",
    "- Creates scaled versions of features for algorithms that need normalization\n",
    "\n",
    "### Models Trained:\n",
    "1. **Random Forest**: Ensemble of decision trees, handles mixed data types well\n",
    "2. **Gradient Boosting**: Sequential learning, builds strong classifier from weak ones\n",
    "3. **Logistic Regression**: Linear model, uses scaled features\n",
    "4. **Support Vector Machine (SVM)**: Finds optimal decision boundary, uses scaled features\n",
    "\n",
    "### For Each Model:\n",
    "- Fits the model on training data\n",
    "- Makes predictions on test data\n",
    "- Calculates performance metrics (accuracy, precision, recall, F1-score, ROC-AUC)\n",
    "- Extracts feature importance (which features matter most)\n",
    "- Stores all results in `ModelResults` objects\n",
    "\n",
    "## 7. **Model Evaluation Phase** (`evaluate_models()`)\n",
    "- Compares all models side-by-side in a performance table\n",
    "- Ranks models by F1-score (balance of precision and recall)\n",
    "- Identifies the best performing model\n",
    "- Prints detailed comparison showing strengths/weaknesses\n",
    "\n",
    "## 8. **Model Persistence Phase** (`save_models()` & `save_best_model()`)\n",
    "- **Saves all trained models** as `.pkl` files using joblib\n",
    "- **Saves the feature scaler** (needed for future predictions)\n",
    "- **Saves metadata** including:\n",
    "  - Training date and time\n",
    "  - Feature names and their order\n",
    "  - Model performance metrics\n",
    "  - Preprocessing steps used\n",
    "\n",
    "## 9. **Prediction Capability** (`predict_collection()`)\n",
    "For new NFT collections:\n",
    "- Takes raw collection features as input\n",
    "- Applies the same feature engineering transformations\n",
    "- Uses the saved scaler if needed (for Logistic Regression/SVM)\n",
    "- Returns prediction with confidence scores:\n",
    "  - `is_legitimate`: Binary prediction (True/False)\n",
    "  - `legitimacy_probability`: Confidence in legitimacy (0-1)\n",
    "  - `risk_score`: Inverse of legitimacy (1 - legitimacy_probability)\n",
    "  - `confidence`: Model's certainty in its prediction\n",
    "\n",
    "## 10. **Main Execution Flow** (`main()`)\n",
    "When run as a script:\n",
    "1. Creates `NFTAuthenticityModel` instance\n",
    "2. Loads and prepares training data\n",
    "3. Generates synthetic data if dataset is too small\n",
    "4. Trains all four models\n",
    "5. Evaluates and compares performance\n",
    "6. Saves the best model for future use\n",
    "7. Provides success/error feedback\n",
    "\n",
    "## Key Design Decisions:\n",
    "\n",
    "**Why Multiple Models?** Different algorithms have different strengths - ensemble methods like Random Forest handle complex interactions, while linear models like Logistic Regression are interpretable.\n",
    "\n",
    "**Why Synthetic Labels?** NFT authenticity is subjective and ground truth is rare. The heuristic approach uses observable market signals that correlate with legitimacy.\n",
    "\n",
    "**Why Feature Engineering?** Raw features like \"total_volume\" become more meaningful when combined (e.g., \"volume_per_owner\" indicates liquidity quality).\n",
    "\n",
    "**Why Data Augmentation?** Machine learning needs sufficient data to learn patterns. Synthetic data generation ensures minimum viable dataset size while maintaining realistic feature distributions.\n",
    "\n",
    "The model essentially learns to recognize patterns that distinguish established, actively-traded NFT collections from potentially suspicious or low-quality ones based on market activity, social presence, and verification status.\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Stack:\n",
    "- **Python Libraries**: pandas, numpy, scikit-learn, joblib\n",
    "- **ML Algorithms**: Random Forest, Gradient Boosting, Logistic Regression, SVM\n",
    "- **Data Sources**: OpenSea API, Reddit API\n",
    "- **Output Format**: Pickled models (.pkl) with JSON metadata"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
